---
title: "Valuating Monitoring Designs"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: "Missouri River Pallid Sturgeon Technical Team"
output:  
  html_document:
    theme: readable
    highlight: tango
  pdf_document:
    highlight: zenburn 
  word_document:
    highlight: monochrome
    fig_width: 6.5  
    fig_height: 6.5 
    reference_docx: tmp-editing.docx
---

<!--
To do:
1. utility for model
-->

```{r,echo=FALSE,warning=FALSE, message=FALSE, eval=TRUE}
source("_R/6-analysis-cost.R")
pbias<- c(-300:300)
pbias_u<- (max(abs(pbias))-abs(pbias))/(max(abs(pbias))-min(abs(pbias)))
u_bias<- approxfun(pbias,pbias_u)
pbias<- c(10:300)
pbias_u<- (max(pbias)-pbias)/(max(pbias)-min(pbias))
u_cv<- approxfun(pbias,pbias_u)
pbias<- c(30:100)
pbias_u<- (pbias-min(pbias))/(max(pbias)-min(pbias))
u_perf<- approxfun(pbias,pbias_u)
```

## Valuation of monitoring design utility

### Objectives hierarchy and attributes

The objectives identified by stakeholders can be valued in 
varying ways which in turn are used to calculate the 
value of a monitoring design. Some objectives 

The numbered objectives correspond to fundamental objectives identified 
during the workshop. Bulleted lists within each numbered objective in 
bold are measurable attributes that can be used to quantify each 
objective. For example there are 3 attributes under objective 1 that can 
be quantified for each monitoring alternative. Assuming these attributes 
are scaled to a common scale (e.g., 0 to 1, 0 to 100) then each bullet 
may receive a weight of 33% if each attribute is equally important to 
decision makers. Alternatively these values can be weighted to reflect 
perceived importance by decision makers. 




#### 1. Quantify PS recruitment to age-1 (Natural origin)

1. Power to detect age-1 natural origin recruits if recruitment occurs
1. Segment level age-1 abundance
    1. bias
    1. precision
1. Estimate age-1 recruitment rate (natural origin)
    1. bias
    1. precision


#### 2. Quantify PS population trend (natural and hatchery origin)

In simulating population monitoring designs, we are using 3 metrics to 
quantify how a monitoring design meets the objective of _quantifying 
population trend_. Specifically, the estimates from a monitoring 
program, as it relates to trend are evaluated by estimating basin level 
population growth rate $\lambda$. 

1. **Bias: How does a trend estimate compare to the true trend.** Bias 
is calculated as the true value minus the estimated value. We then 
divide the bias by the true value to be able to combine estimates of 
varying magnitude (i.e., survival, abundance), recall the previous post 
that used proportional bias. 



2. **Precision: How precise are estimated trend values.** Precision is 
specified as the coefficient of variation (CV) calculated as the 
standard error of the estimate divided by the parameter estimate. There 
is no real threshold for what is optimal for estimator precision 
regarding decision making. Generally speaking, the more precise an 
estimate, the better. There are other alternatives to CV. However, CV is 
commonly used in fisheries and therefore likely to be familiar. 



3. **Performance: How likely is an estimator successful.** In some 
cases, an estimator like the Robust Design may not have enough 
information for the estimator to provide an actual estimate. This 
measure is quantified as the proportion of stochastic simulations where 
the estimator did not converge, or convergence was problematic. Let's 
step through an example to clarify exactly what we are talking about. 
Suppose we have randomly generated 200 Pallid Sturgeon Populations. Then 
we simulate 2 alternative monitoring programs, a catch effort program 
and a capture recapture program. Then we estimate trend from the 
estimates from the 2 designs. In the case of a catch effort based 
monitoring program, the performance is 100% because there are no 
instances where trend cannot be estimated from CPUE data, albeit zeros 
can be an issue at low abundances or capture probability, but that does 
not preclude us from calculating CPUE. However, if capture probability 
is low, then there may be instances where a capture recapture estimator 
just does not work, and estimates cannot be made because the capture 
recapture histories are just too sparse.


#### 3.  Maintain compatibility with legacy PSPAP data

1. Proportion of randomly selected bends within segment
1. Gears similarity: proportion of standard gears used by design
1. Effort similarity: deviation from average effort

#### 4. Provide relevant PS model inputs


#### 5. ## Remain in cost constraints 

Cost is the ultimate PSPAP constraint. Designs exceeding cost 
containments will not be considered. Significant uncertainty exists in 
costs because of several factors. First, field crews tend to become more 
efficient over time and therefore recent PSPAP costs may not represent 
the actual costs. However, if the next iteration of the PSPAP uses 
similar sampling units (i.e., bends within segments) to the current 
PSPAP, then costs for field sampling will likely be similar. 



Minimize costs and stay within cost constraints. The expected cost for 
each design will be calculated. The expected cost can be used 2 ways. 
First it can screen for designs that exceed cost constrains. Second, it 
can be used in an absolute sense to quantify design cost. Lastly,it can 
be used relatively by dividing the value by the expected cost of the 
current PSPAP and therefore values less than 1 are cheaper and values 
greater than 1 are more expensive. 1. Designs exceeding cost constraints 
not considered. 



##### Estimating cost per gear deployment

Estimating cost for a day of sampling is difficult. It is the function 
of several factors, including but not limited to the number of 
personnel, pay rates, other personnel expenses, travel, time sampling, 
and gear maintenance. Additionally, the cost of sampling varies among 
years and costs may be nonlinearly related if there is economy of scale 
(i.e., it may cost less per sampling unit if many are done). 



There are 7 field offices that have conducted PSPAP sampling since 2003. 
Annually the USACE provides funding to these field offices to perform 
PSPAP sampling. Annual values vary from 




1. Annual funding provided to each field crew
1. Calculate the number of bends sampled in a year
1. Divide the cost by the total to get a field crew specific
cost per bend. 
1. Model or empirical distribution



* The season will begin when water temperatures decline to 12.8C or less 
(in the fall) and will continue through June 30. 
* The Fish Community Season will be July 1 through October 30 throughout 
the geographic range of the PSPAP. 




#### Important assumptions

Currently the analysis assumes that the period of performance for field 
crews is the same as the fiscal year. Discussions with USACE contracting 
and PSPAP personnel suggested this was a reasonable assumption. 



This was an example outlining how alternative monitoring programs can be 
objectively evaluated and compared in the context of meeting agency 
objectives. Many uncertainties remain in Pallid Sturgeon population 
dynamics and capture that will need to be accounted for. Additionally, 
the weighting of utility values can be a treacherous territory and how 
metrics are weighted can drive outcomes. However, sensitivity analyses 
can be conducted to evaluate the influence of weighting on outcomes. 
This process of evaluating alternative monitoring programs is designed 
purposely to be as objective as possible and therefore formally linking 
the outcomes of alternative monitoring designs to agency objectives with 
quantifiable metrics is necessary. 
  




1. Survival (RPMA level)
    1. bias
    1. precision
1. Fecundity (RPMA level)
    1. bias
    1. precision
1. Growth (RPMA)
    1. bias
    1. precision
1. Movement
    1. fidelity
    1. among segment movement
1. Population structure and characteristics (segment level)
    1. Size structure
        1. bias
        1. precision
    1. Sex ratio (segment level)
        1. bias
        1. precision


### 3. Estimate segment-level abundance, origin and stage specific

1. bias

1. precision

1. spatial distribution
 



### Valuing the fundamental objective

The 3 metrics described above can be combined into a single 
metric---commonly referred to as a utility---representing the objective 
to _quantify population trend_ [@RN4402]. The utility is then used to 
valuate alternative monitoring programs. However, one problem we run 
into with the metrics above is that they are on different scales. Bias 
can be negative or positive with values approaching 0 being best, 
precision is a positive number varying from 0 (best) to potentially 
large numbers (worst), and reliability is constrained between 0 (worst) 
and 1 (best). 



To convert the 3 metrics to a common scale we can use methods like 
proportional scaling which normalized values to a specified minimum and 
maximum. For example, we can scale the bias to values varying from 0 to 
1 as: 



$$U=\frac{|bias|-max(|bias|)}{max(|bias|)-min(|bias|)},$$

where $|bias|$ is the absolute value of bias. We use absolute value here 
because we are assuming negative and positive bias are equally bad 
regarding satisfying the objective to _quantify population trend_. In 
the plot below, values with lower proportional bias are given higher 
values, and increasing values approach 0. 




 

Now let's look at the precision metric. Suppose it varies from 10 to 300 
(not very precise). The equation to calculate scaled precision is the 
same as before. However, we do not need to take absolute values since 
all values are positive. In the plot below, values with lower CV values 
have higher precision utility values, and precision utility approaches 0
as CV values increase to the maximum. 

<!--Lastly, let's look at performance. Suppose performance values from 
simulating multiple replicates of each monitoring design vary from 35% 
to 100%. The difference between the performance metric with bias and 
precision is that higher values are more desirable and therefore we need 
to rearrange the proportional scaling equation to reflect this as, 
-->
 ADD IN SOMETHING ABOUT BIAS AND PRECISION ESTIMATES BEING CONDITIONAL 
 ON RELIABILITY/PERFORMANCE AND HOW WE DEAL WITH THIS



### Linking monitoring designs and objectives

NEED TO CHANGE AROUND DUE TO NEW WAY OF HANDLING RELIABILITY/PERFORMANCE 
AND MAKE A NOTE ON RELATIVE VS ABSOLUTE

We can combine the utility values now such that monitoring designs 
resulting in a trend estimate with relatively low bias, relatively high 
precision, and good performance have higher values (i.e., approaching 1) 
and estimates of the trend that is biased, imprecise with poor 
performance approach 0. 
Values approach 1 because each of the scaled metrics is weighted. For 
example, if each metric is valued equally, then the weights would be 
1/3. Alternatively, if really precise estimates of trend were desired 
the weight for scaled trend could be 0.5 and the remaining metrics 
weighted at 0.25. Suppose this last weighting scheme is the case and the 
output from 2 monitoring programs, a catch effort based and capture 
recapture program, are the values below. 

* Catch effort
    * Proportional bias = -60, scaled = `r round(u_bias(-60),3)`
    * Precision = 112, scaled = `r round(u_cv(112),3)`
    * Performance = 100, scaled = `r round(u_perf(100),3)`
* Capture recapture
    * Proportional bias = 5, scaled = `r round(u_bias(5),3)`
    * Precision = 115, scaled = `r round(u_cv(115),3)`
    * Performance = 90, scaled = `r round(u_perf(90),3)`

The scaled utility for the catch effort program is:


$$`r round(0.25*u_bias(-60)+0.50*u_cv(112)+0.25*u_perf(100),3)`= 
0.25\cdot `r round(u_bias(-60),3)`+
0.50 \cdot `r round(u_cv(112),3)`+
0.25\cdot`r round(u_perf(100),3)`,$$


and the scaled utility for the capture recapture program is 

$$`r round(0.25*u_bias(5)+0.50*u_cv(115)+0.25*u_perf(90),3)`= 
0.25\cdot `r round(u_bias(5),3)`+
0.50 \cdot `r round(u_cv(115),3)`+
0.25\cdot`r round(u_perf(90),3)`.$$

The combined utility values indicate that the capture recapture program 
has slightly more value to achieve the objective of _quantifing 
population trend_. 





$$U=\frac{Performance-min(Performance)}{max(Performance)-min(Performance)}.$$



## Swing weighting
The challenge in combining utiltities is determining how each 
performance metric utility should be weighted.  To understand some of 
the complications assume that in general we feel trend precision is 
more important than trend bias.  Choosing a weight of 0.75 
for precision and 0.25 for bias, based on this feeling as we did in the 
example above, is not only a very subjective approach but it also did 
not take into account variation in the metrics---something 
that should affect the weighting.  For example, if all monitoring 
designs had trend estimates with CV values that ranged between 0.001 and 
0.002 (i.e., great precision), then we'd be happy with the precision of 
any of our choices.  Hence, if trend bias values varied greatly, when 
making a decision we would focus on which choice of monitoring design 
also led to a small bias.  In this case, the bias metric should be 
weighted higher than the precision metric (eventhough in general we 
think precision is more important than bias) because the bias values 
should have a greater impact on the choice of monitoring design given 
the small variation in precision values.  To curtail these complications 
we used swing weighting to determe metric utility weights.  

Swing weighting, as a structured approach, is less subjective and takes 
into account variation.  The process involves comparing the worst case 
to the best case scenario for each metric while holding each of the 
other metrics constant at their worst case.  In other words, swing 
weighting forces you to think about how your decision is impacted when 
each metric "swings" from worst to best.  These "swings" are first 
ranked and then valued.  The swing that has the most impact on the 
decision process (relative to the other swings) would be given a rank of 
1 (most important), the swing with the second greatest impact on the 
decision a rank of 2, and so on. The baseline scenario, where all 
metrics are set to their worst case, always gets the highest (worst) 
rank.  The scenario with a rank of 1 is assigned a value of 100 and the 
baseline scenario is assigned a value of 0.  All other cases are then 
given a value from 0 to 100, representing the impact they have on the 
decision making process relative to the highest and lowest ranked 
scenarios.

In the case of trend described a couple paragraphs ago, 
lets say the trend bias varies from 1%-70%. Then, as trend CV values 
swing from 0.002 (worst) to 0.001 (best), there is little impact on our 
decision process relative to when trend bias values swing from 70% 
(worst) to 1% (best).  Therefore, we would rank the scenario with the 
best bias with a 1 and the scenario with the best precision with a 2 
(see Figure/Table ??).  The scenario with the best bias would get a 
value of 100, and the scenario with the best precision will get a rank 
from 0 to 100---0 if I think having the best precision relative to the 
worst precsion has no impact on the decision (i.e., its just as bad as 
thebaseline scenario) and 100 if I think having the best precision has 
just as much impact on the decision as the swing in bias does. For 
the sake of example, assume I think the impact of the precision 
swing is about 1/10 the impact of the bias swing.  Then I would value 
the scenario with the best precision with a 10.  It is important to 
note, that this does not mean that precision is weighted by 0.10; it just 
means that having the best precision and the worst bias is worth about 
1/10 as having the worst precision and the best bias. Instead, the 
weights are determined by dividing each value by the sum of all 
values---since any case having both the best precision and the best bias[^1] 
would end up with the highest value of 110 or a utility weight of 1, as desired.  

[^1] Under the swing weighting assumption that the impact on the 
decision is a linear function of the metrics 


### Stakeholder Input
Inevitably, individual stakeholders, as well as different stakeholder 
agencies, will have different opinions on what metrics should have a 
greater impact on the choosing a monitoring design.  As a consistent, 
structured process, swing weighting provides an excellent approach for 
comparing and compiling stakeholder weights.  We are currently in the 
process of using swing weighting as a way to understand what differences 
and similarities there are in stakeholder opinions on the importance of 
objective metrics and the consequences of these on valuating various 
monitoring designs.  While PSPAP v. 2.0 will not be making any decisions 
based on these weights, this analysis will provide valuable stakeholder 
information to and be a tool for decision makers.

In order for swing weighting to be used consistently amongst 
stakeholders, it is important for each stakeholder to have a good 
understanding of the swing weighting process.  We created YouTube videos
that described the swing weighting process and its role in the PSPAP v. 
2.0 {cite swing weighting tutorial video} and explained the meaning and 
variation in the abundance and trend metrics {cite Abundance and Trend 
videos}.  We posted a downloadable swing weighting form, links to the 
YouTube videos described above, and instructions on how to fill in the 
swing weighting form to the PSPAP v. 2.0 blog {cite swing weighting 
blog page}.  An email was also sent out to stakeholders inviting them
to visit the blog to fill in and submit a swing weighting form.  Once  
swing weighting data has been collected, valuation of monitoring 
designs can be considered across the range of stakeholder weighting 
schemes, as well as looked at by stakeholder angency.  



<!--
Quantify PS abundance or relative abundance	 
Quantify catch rates of age 0 and age 1 PS	 
Age at maturity	 
Age structure	 
Blood	 

Catch effort	 
Competition with invasive species	 
Contaminants	 
Didson	 
Diet	 
Disease	 
Drift and dspersal	 
 
Egg quality	 
Egg sample	 
Estimate Effective population size	 
Evaluate annual trends in Native forage fish	 
Fecundity	 
Fin ray	 
Fish communities	 
Fish condition	 
Foraging habitat	 
Free embryo collection	 
Genetic composition	 
Growth	 
Habitat selection	 
 
Hybridization	 
	 
IRC habitat	 
 
Local adaptation	 
 
Microchemistry	 
Model-based estimates of abundance of age 0 and age 1 PS	 
Model-based estimates of survival of hatchery and naturally reproducing PS to age 1	 
Movement	 
	 
Population estimates for PS for all size and age classes, particularly ages 2 to 3	 
Population structure and other characteristics	 
Predation	 
RNA Stress markers	 
Reproductive cycling	 
Reproductive readiness	 
Robust design	 
Sex	 
Sex ratio	 
Spawning aggregation and synchrony	 
Spawning habitat	 
Stable isotopes	 
Stocking program reports	 
 
Stress	 
Survival	 
	 
	 
	 
Use of Mississippi and tributaries	 
Zooplankton density	 
catch rates of all PS by size class	 


Hydroacoustic monitoring
N mixture model	 
PIT tag fish
Trawling
ultrasound
Telemetry River sweep
CJS	 
Calibrated Population Model	
Measure fish length and weight
Take a Tissue sample	
Lavage	 
Stomach removal	
EDNA	
Hatchery report and data	
Closed population estimators
Smaller gill mesh
Use trammel nets
-->
