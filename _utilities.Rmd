---
title: "White Paper
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: "Missouri River Pallid Sturgeon Technical Team"
output:  
  html_document:
    theme: readable
    highlight: tango
  pdf_document:
    highlight: zenburn 
  word_document:
    highlight: zenburn
---

<!--
To do:
1. utility for model
-->


# Valuing fundamental objectives

Let's look at how we can quantify one of the fundamental objectives
for the PSPAP going forward, specifically the objective to:

_Quantify population trend_.






# Fundamental objectives

## Quantify PS recruitment to age 1.

### Bias

### Precision

### Performance



## Provide estimates with associated uncertainty needed to run the PS population model



## Quantify PS population trend


In simulating population monitoring designs, we are using 3
metrics to quantify how a monitoring design meets the objective of _quantifying
population trend_. Specifically, the 
estimates from a monitoring program, as it relates to trend are evaluated 
by calculating:  

1. **Bias: How does an estimate compare to the true value.** Bias is 
calculated as the true value minus the estimated value. We then divide 
the bias by the true value to be able to combine estimates of varying 
magnitude (i.e., survival, abundance), recall the previous post that 
used proportional bias. 
2. **Precision: How precise are estimated values.** Precision is 
specified as the coefficient of variation (CV) calculated as the 
standard error of the estimate divided by the parameter estimate. There 
is no real threshold for what is optimal for estimator precision 
regarding decision making. Generally speaking, the more precise an 
estimate, the better. There are other alternatives to CV. However, CV is 
commonly used in fisheries and therefore likely to be familiar. 
3. **Performance: How likely is an estimator successful.** In some 
cases, an estimator like the Robust Design may not have enough 
information for the estimator to provide an actual estimate. This 
measure is quantified as the proportion of stochastic simulations where 
the estimator did not converge, or convergence was problematic. Let's 
step through an example to clarify exactly what we are talking about. 
Suppose we have randomly generated 200 Pallid Sturgeon Populations. Then 
we simulate 2 alternative monitoring programs, a catch effort program 
and a capture recapture program. Then we estimate trend from the 
estimates from the 2 designs. In the case of a catch effort based 
monitoring program, the performance is 100% because there are no 
instances where trend cannot be estimated from CPUE data, albeit zeros 
can be an issue at low abundances or capture probability, but that does 
not preclude us from calculating CPUE. However, if capture probability 
is low, then there may be instances where a capture recapture estimator 
just does not work, and estimates cannot be made because the capture 
recapture histories are just too sparse! 


### Valuing the fundamental objective

The 3 metrics described above can be combined into a single 
metric---commonly referred to as a utility---representing the objective 
to _quantify population trend_ [@RN4402]. The utility is then used to 
evaluate alternative monitoring programs. However, one problem we run 
into with the metrics above is that they are on different scales. Bias 
can be negative or positive with values approaching 0 being best, 
precision is a positive number varying from 0 (best) to potentially 
large numbers (worst), and conformance is constrained between 0 (worst) 
and 100 (best). 


To convert the 3 metrics to a common scale we can use methods like proportional scaling
which normalized values to a specified minimum and maximum. For example, we can scale
the bias to values varying from 0 to 1 as:

$$U=\frac{|bias|-max(|bias|)}{max(|bias|)-min(|bias|)},$$

where $|bias|$ is the absolute value of bias. We use absolute value here 
because we are assuming negative and positive bias are equally bad 
regarding satisfying the objective to _quantify population trend_. In 
the plot below, values with lower proportional bias are given higher 
values, and increasing values approach 0. 



```{r,echo=FALSE,fig.align='center'}
pbias<- c(-300:300)
pbias_u<- (max(abs(pbias))-abs(pbias))/(max(abs(pbias))-min(abs(pbias)))
u_bias<- approxfun(pbias,pbias_u)
plot(pbias_u~pbias,type='l',xlab="Proportional bias",
    ylab="Scaled proportional bias",las=1)
```

 

Now let's look at the precision metric. Suppose it varies from 10 to 300 
(not very precise). The equation to calculate scaled precision is the 
same as before. However, we do not need to take absolute values since 
all values are positive. In the plot below, values with lower CV values 
have higher values, and increasing values approach 0. 



```{r,echo=FALSE,fig.align='center'}
pbias<- c(10:300)
pbias_u<- (max(pbias)-pbias)/(max(pbias)-min(pbias))
u_cv<- approxfun(pbias,pbias_u)
plot(pbias_u~pbias,type='l',xlab="Scaled coefficient of variation",
    ylab="Scaled coefficient of variation",las=1)
```


Lastly, let's look at performance. Suppose performance values from simulating
multiple replicates of each monitoring design vary from 35% to 100%. The difference
between the performance metric with bias and precision is that higher values are more
desirable and therefore we need to rearrange the proportional scaling equation to reflect
this as, 

$$U=\frac{Performance-min(Performance)}{max(Performance)-min(Performance)}.$$

In the plot below, values with lower performance values have lower 
values and increasing values approach 1. 


```{r,echo=FALSE,fig.align='center'}
pbias<- c(30:100)
pbias_u<- (pbias-min(pbias))/(max(pbias)-min(pbias))
u_perf<- approxfun(pbias,pbias_u)
plot(pbias_u~pbias,type='l',xlab="Performance",
    ylab="Scaled performance",las=1)
```



### Bias

### Precision

### Performance


## Maintain compatibility with legacy data

* number of random bends selected - incorporate gear type?


## Remain in cost constraints 

Cost is the ultimate PSPAP constraint. Designs exceeding cost 
containments will not be considered. Significant uncertainty exists in 
costs because of several factors. First, field crews tend to become more 
efficient over time and therefore recent PSPAP costs may not represent 
the actual costs. However, if the next iteration of the PSPAP uses similar 
sampling units (i.e., bends within segments) to the current PSPAP, 
then costs for field sampling will likely be similar. 


### Estimating cost to sample a bend

Estimating cost for a day of sampling is difficult. It is the function 
of several factors, including but not limited to the number of 
personnel, pay rates, other personnel expenses, travel, time sampling, 
and gear maintenance. Additionally, the cost of sampling varies among years
and costs may be non linearly related if there is economy of scale (i.e.,
it may cost less per sampling unit if many are done). 

1. Annual funding provided to each field crew
1. Calculate the number of bends sampled in a year
1. Divide the cost by the total to get a field crew specific
cost per bend. 
1. Model or empirical distribution


* The season will begin when water temperatures decline to 55Â°F or less 
(in the fall) and will continue through June 30. 
* The Fish Community Season will be July 1 through October 30 throughout 
the geographic range of the PSPAP. 

### Important assumptions

Currently the analysis assumes that the period
of performance for field crews is the same as 
the fiscal year. 

This was an example outlining how alternative monitoring programs can be 
objectively evaluated and compared in the context of meeting agency 
objectives. Many uncertainties remain in Pallid Sturgeon population 
dynamics and capture that will need to be accounted for. Additionally, 
the weighting of utility values can be a treacherous territory and how 
metrics are weighted can drive outcomes. However, sensitivity analyses 
can be conducted to evaluate the influence of weighting on outcomes. 
This process of evaluating alternative monitoring programs is designed 
purposely to be as objective as possible and therefore formally linking 
the outcomes of alternative monitoring designs to agency objectives with 
quantifiable metrics is necessary. 
  

```{r,echo=FALSE,warning=FALSE, message=FALSE}

source("_R/6-analysis-cost.R")
hist(depcost[depcost$fieldoffice=="NE",]$perdep)
hist(depcost[depcost$fieldoffice=="CF",]$perdep)

plot(lperdep~year,depcost,
    type='n')
    
    plot(resid(fit)~fitted(fit))
plot(lperdep~ldepcosthat,depcost)    
plot(perdep~year,depcost,
    subset=fieldoffice=="SD")
    
plot(perdep~year,depcost,
    subset=fieldoffice=="MT")
    
plot(perdep~year,depcost,
    subset=fieldoffice=="CF")  
``` 





Quantify PS abundance or relative abundance	 
Quantify catch rates of age 0 and age 1 PS	 
Age at maturity	 
Age structure	 
Blood	 

Catch effort	 
Competition with invasive species	 
Contaminants	 
Didson	 
Diet	 
Disease	 
Drift and dspersal	 
 
Egg quality	 
Egg sample	 
Estimate Effective population size	 
Evaluate annual trends in Native forage fish	 
Fecundity	 
Fin ray	 
Fish communities	 
Fish condition	 
Foraging habitat	 
Free embryo collection	 
Genetic composition	 
Growth	 
Habitat selection	 
 
Hybridization	 
	 
IRC habitat	 
 
Local adaptation	 
 
Microchemistry	 
Model-based estimates of abundance of age 0 and age 1 PS	 
Model-based estimates of survival of hatchery and naturally reproducing PS to age 1	 
Movement	 
	 
Population estimates for PS for all size and age classes, particularly ages 2 to 3	 
Population structure and other characteristics	 
Predation	 
RNA Stress markers	 
Reproductive cycling	 
Reproductive readiness	 
Robust design	 
Sex	 
Sex ratio	 
Spawning aggregation and synchrony	 
Spawning habitat	 
Stable isotopes	 
Stocking program reports	 
 
Stress	 
Survival	 
	 
	 
	 
Use of Mississippi and tributaries	 
Zooplankton density	 
catch rates of all PS by size class	 


Hydroacoustic monitoring
N mixture model	 
PIT tag fish
Trawling
ultrasound
Telemetry River sweep
CJS	 
Calibrated Population Model	
Measure fish length and weight
Take a Tissue sample	
Lavage	 
Stomach removal	
EDNA	
Hatchery report and data	
Closed population estimators
Smaller gill mesh
Use trammel nets



