---
title: "PSPAP Rebood Methods Overview"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: "Missouri River Pallid Sturgeon Technical Team"
output:  
  html_document:
    theme: readable
    highlight: tango
  pdf_document:
    highlight: zenburn 
  word_document:
    highlight: monochrome
    fig_width: 6.5  
    fig_height: 6.5 
    reference_docx: tmp-editing.docx
---

<!--
rmarkdown::render("methods-overview.Rmd",
    output_format="word_document")# build page
-->

```{r, warning=FALSE, include=FALSE}
source("_R/1_global.R")
```

## Methods overview

The approach used to evaluate alernative monitoring designs was intended 
to be rigorous and transparent because of the many stakeholders effected 
by potential modifications to the existing PSPAP program. Additionally,
the methods used will provide a tool for the U.S. Army Corp of Engineers 
(USACE) to make transparent decisions in the face of uncertain future 
monitoring funding levels that will likely result in difficult future 
decisions.  

We used a structured decision making approach to elicit stateholder 
objectives, simulation modeling to generate reference populations, 
alternative monitoring designs to estimate relevant population metrics, 
and valuation of stakeholder objectives to quantify the utility of 
monitoring design alternatives. The methods are covered in 6 sections 
detailing 

1. Elicitation of stakeholder objectives,
2. Generation of reference populations with known dynamics (e.g., 
recruitment, survival), 
3. Generation of catch data from known catchabilities and efforts given alternative monitoring designs,
4. Estimation of population metrics given alternative monitoring designs,
5. Valuation of monitoring design utility, and
6. Routing of monitoring design subobjectives.

<!--NEED TO LINK TO BDN DESCRIPTION ABOVE OR ALTER INTRO...(BDN NOT MENTIONED HERE, BUT MENTIONED IN INTRO)-->

## Eliciting stakeholder objectives
### 2017 MRNRC workshop
```{r echo=FALSE, results='asis'}
cat(knit_child(text = readLines('_Report-1-01-introduction.Rmd')[181:215], quiet=TRUE), sep = '\n')
```

<!--TALK ABOUT UPDATE WEBINARS AND JANUARY MEETING AND PRUNING OVERALL OBJECTIVES...ANCILLARY OBJECTIVES DISCUSSIONS HERE???
### PSPAP update at 2017 Fall Science Meeting -- is this something we should include?
### PSPAP January 2018 Update
-->

### Objectives hierarchy and attributes

The numbered objectives correspond to fundamental objectives identified 
during the workshop. Bulleted lists within each bold, numbered objective 
are measurable attributes that can be used to quantify each objective. 
For example there are 3 attributes under objective 1 that can be 
quantified for each monitoring alternative. Assuming these attributes 
are scaled to a common scale (e.g., 0 to 1, 0 to 100) then each bullet 
may receive a weight of 33% if each attribute is equally important to 
decision makers. Alternatively these values can be weighted to reflect 
perceived importance by decision makers (see "Valuation of monitoring 
design utility" for details on combining metrics). 

<!--Add description of square bullets-->

1. **Detect and Quantify PS recruitment to age-1 (natural origin)**
    * Detection    
        * Power to detect age-1 natural origin recruits if recruitment occurs
        * Estimator reliability
    * Age-1 Abundance  
        * Population model back calculation estimates
              * bias
              * precision
              * reliability
2. **Quantify PS population trend and abundance (natural and hatchery origin)**
    * Trend: Estimate RPMA level population growth rate $\lambda$ <!--TW: At some point, we will need to specify a time period over which this will be measured.  It will affect the design. -->
        * bias
        * precision
        * estimator reliability
    * Abundance: Estimate RPMA level population abundance $N$ each year
        * bias
        * precision
        * estimator reliability
3. **Provide relevant PS model inputs**
    * Abundance:  Estimate segment level abundance, origin and stage specific 
        * bias
        * precision
        * estimator reliability
        * spatial distribution <!--within segment? to what level? (bend, finer scale?)  Seems like this would just be a second parameter estimate with its own bias, precision, and reliability.-->
    * Survival: Estimate average annual survival at RPMA level
        * bias
        * precision
        * estimator reliability
    * Fecundity: Estimate average fecundity at RPMA level
        * bias
        * precision
        * estimator reliability
    * Growth:  <!--Estimate ... at--> (RPMA level)  <!--Do we just have one estimate here, or do we need to estimate  k and Linf? or mean and sd of a bivariate normal?-->
        * bias 
        * precision
        * estimator reliability
    * Movement
        * Site fidelity
            * bias 
            * precision
            * estimator reliability
        * Among segment movement
            * bias 
            * precision
            * estimator reliability
    * Size structure:  Estimate proportion of individuals in each size class at the segment level
        * bias 
        * precision
        * estimator reliability
    * Sex ratio:  Estimate sex ratio at the segment level
        * bias
        * precision
        * estimator reliability
4. **Maintain compatibility with legacy PSPAP data** <!-- TW: If this adds to the cost, then the benefit will need to be explained.  Need to indicate (at some point) why this is helpful.  -->
    * Proportion of randomly selected bends within segment
    * Gears similarity: proportion of standard gears used by design
    * Effort similarity: deviation from average effort <!--What is the  reason behind this bullet?  CPUE can be calculated for any non-zero level of effort... Is it because N and CPUE are not really a linear relationship and straying too far from the normal effort level can cause issues with CPUE comparisions?-->
5. **Stay below cost constraints**
    * Minimize costs
    
  

  